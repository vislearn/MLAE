{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from tqdm import trange\n",
    "from time import time\n",
    "\n",
    "\n",
    "from mlae.evaluate import load_model, get_n_samples_from_dataloader, all_conditions\n",
    "from mlae.evaluate.surrogate_lot_det import (\n",
    "    log_det_surrogate_latent_encoder_mixed,\n",
    "    log_det_surrogate_data_encoder_mixed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gradient(model):\n",
    "    return {\n",
    "        name: deepcopy(param.grad)\n",
    "        for name, param in model.named_parameters()\n",
    "        if param.grad is not None and (param != 0).any()\n",
    "    }\n",
    "\n",
    "\n",
    "def grad_diff(grad1, grad2):\n",
    "    diff_dict = {}\n",
    "    for key in set(grad1) | set(grad2):\n",
    "        try:\n",
    "            value1 = grad1[key]\n",
    "            value2 = grad2[key]\n",
    "            diff_dict[key] = value1 - value2\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return diff_dict\n",
    "\n",
    "\n",
    "def grad_norm(grad):\n",
    "    grad_sum = 0\n",
    "    grad_len = 0\n",
    "    for value in grad.values():\n",
    "        grad_sum = grad_sum + (value**2).sum()\n",
    "        grad_len = grad_len + value.nelement()\n",
    "    return grad_sum.sqrt()\n",
    "\n",
    "\n",
    "def grad_dot(grad1, grad2):\n",
    "    dot = 0\n",
    "    for key in set(grad1) | set(grad2):\n",
    "        try:\n",
    "            value1 = grad1[key]\n",
    "            value2 = grad2[key]\n",
    "            dot = dot + (value1 * value2).sum()\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return dot\n",
    "\n",
    "\n",
    "def gradient_cosine_similarity(grad1, grad2):\n",
    "    grad1_norm = grad_norm(grad1)\n",
    "    grad2_norm = grad_norm(grad2)\n",
    "    grad_prod = grad_dot(grad1, grad2)\n",
    "    return grad_prod / (grad1_norm * grad2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = \"./MLAE/lightning_logs/model_name/version_X\"  # change model_name and version_X\n",
    "\n",
    "model = load_model(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_reps = 2\n",
    "batch_sizes = [1, 16, 64, 256, 512]\n",
    "dim = model.data_dim\n",
    "latent_dim = model.latent_dim\n",
    "max_hutchinson_samples = 16\n",
    "device = \"cuda\"\n",
    "\n",
    "data = []\n",
    "for attempt in trange(experiment_reps):\n",
    "    for batch_size in batch_sizes:\n",
    "        for trace_space in [\"data\", \"latent\"]:\n",
    "            model.to(device)\n",
    "\n",
    "            batch = get_n_samples_from_dataloader(\n",
    "                model.val_dataloader(), batch_size, all_conditions(model)[0]\n",
    "            )\n",
    "            batch, _, _, c_batch = model.apply_conditions(batch)\n",
    "            batch = batch.to(device)\n",
    "            c_batch = c_batch.to(device)\n",
    "\n",
    "            # compute reference gradients\n",
    "            reference_duration = []\n",
    "            reference_gradient = []\n",
    "            reference_hutchinson_samples = []\n",
    "            model.zero_grad()\n",
    "            start = time()\n",
    "            reference_result = log_det_surrogate_latent_encoder_mixed(\n",
    "                batch,\n",
    "                c_batch,\n",
    "                model.encode,\n",
    "                model.decode,\n",
    "                max_hutchinson_samples,\n",
    "            )[-2]\n",
    "            reference_result.mean().backward()\n",
    "            reference_duration.append(time() - start)\n",
    "            reference_gradient.append(collect_gradient(model))\n",
    "            reference_hutchinson_samples.append(max_hutchinson_samples)\n",
    "\n",
    "            # compute surrogate gradient approximations\n",
    "            for hutchinson_samples in range(1, latent_dim + 1):\n",
    "                for orthogonalize_hutchinson_samples in [True, False]:\n",
    "                    try:\n",
    "                        model.zero_grad()\n",
    "                        start = time()\n",
    "                        result = eval(f\"log_det_surrogate_{trace_space}_encoder_mixed\")(\n",
    "                            batch,\n",
    "                            c_batch,\n",
    "                            model.encode,\n",
    "                            model.decode,\n",
    "                            hutchinson_samples,\n",
    "                            orthogonalize=orthogonalize_hutchinson_samples,\n",
    "                        )[-2]\n",
    "                        result.mean().backward()\n",
    "                        duration = time() - start\n",
    "                        gradient = collect_gradient(model)\n",
    "\n",
    "                        for (\n",
    "                            reference_duration_i,\n",
    "                            reference_gradient_i,\n",
    "                            reference_hutchinson_samples_i,\n",
    "                        ) in zip(\n",
    "                            reference_duration,\n",
    "                            reference_gradient,\n",
    "                            reference_hutchinson_samples,\n",
    "                        ):\n",
    "                            data.append(\n",
    "                                {\n",
    "                                    \"trace_space\": trace_space,\n",
    "                                    \"dim\": dim,\n",
    "                                    \"latent_dim\": latent_dim,\n",
    "                                    \"attempt\": attempt,\n",
    "                                    \"batch_size\": batch_size,\n",
    "                                    \"reference_duration\": reference_duration_i,\n",
    "                                    \"reference_hutchinson_samples\": reference_hutchinson_samples_i,\n",
    "                                    \"hutchinson_samples\": hutchinson_samples,\n",
    "                                    \"orthogonalize_hutchinson_samples\": orthogonalize_hutchinson_samples,\n",
    "                                    \"dist\": (\n",
    "                                        grad_norm(\n",
    "                                            grad_diff(gradient, reference_gradient_i)\n",
    "                                        )\n",
    "                                        / grad_norm(reference_gradient_i)\n",
    "                                    ).item(),\n",
    "                                    \"neg_dot\": (\n",
    "                                        1\n",
    "                                        - grad_dot(gradient, reference_gradient_i)\n",
    "                                        / (\n",
    "                                            grad_norm(gradient)\n",
    "                                            * grad_norm(reference_gradient_i)\n",
    "                                        )\n",
    "                                    ).item(),\n",
    "                                    \"relative_norm\": (\n",
    "                                        grad_norm(gradient)\n",
    "                                        / grad_norm(reference_gradient_i)\n",
    "                                    ).item(),\n",
    "                                    \"duration\": duration,\n",
    "                                }\n",
    "                            )\n",
    "                    except TypeError:\n",
    "                        continue\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "for j, (trace_space, axis) in enumerate(zip([\"data\", \"latent\"], [ax[0], ax[1]])):\n",
    "    for i, batch_size in enumerate(batch_sizes):\n",
    "        df_relevant = df.where(\n",
    "            (df[\"orthogonalize_hutchinson_samples\"] == True)\n",
    "            & (df[\"trace_space\"] == trace_space)\n",
    "            & (df[\"batch_size\"] == batch_size)\n",
    "        ).dropna()\n",
    "        dist_list = [\n",
    "            df_relevant.where(df_relevant[\"attempt\"] == i).dropna()[\"dist\"].to_numpy()\n",
    "            for i in range(experiment_reps)\n",
    "        ]\n",
    "        dist_list = [dist for dist in dist_list if len(dist) != 0]\n",
    "        dist_mean = np.mean(dist_list, axis=0)\n",
    "        dist_std = np.std(dist_list, axis=0)\n",
    "        hutchinson_samples_numeration = np.arange(1, max_hutchinson_samples + 1)\n",
    "        axis.plot(\n",
    "            hutchinson_samples_numeration, dist_mean, label=f\"{batch_size}\"\n",
    "        )\n",
    "        axis.fill_between(\n",
    "            hutchinson_samples_numeration,\n",
    "            dist_mean + dist_std,\n",
    "            dist_mean - dist_std,\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "# non-orthogonalized hutchinson samples\n",
    "df_relevant = df.where(\n",
    "    (df[\"orthogonalize_hutchinson_samples\"] == False)\n",
    "    & (df[\"trace_space\"] == \"latent\")\n",
    "    & (df[\"batch_size\"] == 1)\n",
    ").dropna()\n",
    "dist_list = [\n",
    "    df_relevant.where(df_relevant[\"attempt\"] == i).dropna()[\"dist\"].to_numpy()\n",
    "    for i in range(experiment_reps)\n",
    "]\n",
    "dist_list = [dist for dist in dist_list if len(dist) != 0]\n",
    "dist_mean = np.mean(dist_list, axis=0)\n",
    "dist_std = np.std(dist_list, axis=0)\n",
    "hutchinson_samples_numeration = np.arange(1, max_hutchinson_samples + 1)\n",
    "ax[1].plot(hutchinson_samples_numeration, dist_mean, label=f\"{1}\", linestyle=\"--\")\n",
    "ax[1].fill_between(\n",
    "    hutchinson_samples_numeration,\n",
    "    dist_mean + dist_std,\n",
    "    dist_mean - dist_std,\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "ax[0].set_xlabel(\"number of Hutchinson samples\")\n",
    "ax[1].set_xlabel(\"number of Hutchinson samples\")\n",
    "ax[0].set_ylabel(\"relative gradient distance\")\n",
    "ax[1].set_ylabel(\"relative gradient distance\")\n",
    "ax[0].set_yscale(\"symlog\")\n",
    "ax[1].set_yscale(\"symlog\")\n",
    "tick_locations = [\n",
    "    *np.linspace(0, 1, 11),\n",
    "    *np.linspace(1, 10, 10),\n",
    "    *np.linspace(10, 100, 10),\n",
    "]\n",
    "tick_labels_defined = {0: \"0\", 1: r\"$10^0$\", 10: r\"$10^1$\", 100: r\"$10^2$\"}\n",
    "tick_labels = [\n",
    "    tick_labels_defined[loc] if loc in [0, 1, 10, 100] else \"\" for loc in tick_locations\n",
    "]\n",
    "ax[0].set_yticks(tick_locations, tick_labels)\n",
    "ax[1].set_yticks(tick_locations, tick_labels)\n",
    "ax[0].legend(\n",
    "    title=\"batch size\", handletextpad=0.2, ncols=5, columnspacing=0.6, loc=\"lower left\"\n",
    ")\n",
    "ax[1].legend(\n",
    "    title=\"batch size\", handletextpad=0.2, ncols=6, columnspacing=0.6, loc=\"upper right\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
